{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "\n",
    "with open('groceries_mba.csv', 'r',encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    basket= [x for x in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['citrus_fruit', 'semi-finished_bread', 'margarine', 'ready_soups'], 9835)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basket[0], len(basket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['citrus_fruit', 'semi-finished_bread', 'margarine', 'ready_soups'],\n",
       " ['tropical_fruit', 'yogurt', 'coffee'],\n",
       " ['whole_milk'],\n",
       " ['pip_fruit', 'yogurt', 'cream_cheese_', 'meat_spreads'],\n",
       " ['other_vegetables',\n",
       "  'whole_milk',\n",
       "  'condensed_milk',\n",
       "  'long_life_bakery_product'],\n",
       " ['whole_milk', 'butter', 'yogurt', 'rice', 'abrasive_cleaner'],\n",
       " ['rolls/buns'],\n",
       " ['other_vegetables',\n",
       "  'UHT-milk',\n",
       "  'rolls/buns',\n",
       "  'bottled_beer',\n",
       "  'liquor_(appetizer)'],\n",
       " ['pot_plants'],\n",
       " ['whole_milk', 'cereals']]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basket[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data.txt', 'wb') as f:\n",
    "    pickle.dump(basket, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('words.txt', 'rb') as fp:\n",
    "    wordlist = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('embeddings.txt','rb') as fp:\n",
    "    embeddings = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordlist.append('UNKNOWN_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = np.vstack([embeddings, np.zeros(200)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170, 170)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordlist), len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_to_index = dict([(w,i) for i,w in enumerate(wordlist)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Instant_food_products': 99,\n",
       " 'UHT-milk': 38,\n",
       " 'UNKNOWN_TOKEN': 169,\n",
       " 'abrasive_cleaner': 128,\n",
       " 'artif._sweetener': 131,\n",
       " 'baby_cosmetics': 163,\n",
       " 'baby_food': 167,\n",
       " 'bags': 164,\n",
       " 'baking_powder': 63,\n",
       " 'bathroom_cleaner': 139,\n",
       " 'beef': 26,\n",
       " 'berries': 40,\n",
       " 'beverages': 50,\n",
       " 'bottled_beer': 12,\n",
       " 'bottled_water': 5,\n",
       " 'brandy': 124,\n",
       " 'brown_bread': 18,\n",
       " 'butter': 24,\n",
       " 'butter_milk': 48,\n",
       " 'cake_bar': 74,\n",
       " 'candles': 93,\n",
       " 'candy': 44,\n",
       " 'canned_beer': 14,\n",
       " 'canned_fish': 72,\n",
       " 'canned_fruit': 133,\n",
       " 'canned_vegetables': 81,\n",
       " 'cat_food': 56,\n",
       " 'cereals': 111,\n",
       " 'chewing_gum': 58,\n",
       " 'chicken': 30,\n",
       " 'chocolate': 28,\n",
       " 'chocolate_marshmallow': 91,\n",
       " 'citrus_fruit': 11,\n",
       " 'cleaner': 118,\n",
       " 'cling_film/bags': 78,\n",
       " 'cocoa_drinks': 144,\n",
       " 'coffee': 22,\n",
       " 'condensed_milk': 86,\n",
       " 'cooking_chocolate': 141,\n",
       " 'cookware': 138,\n",
       " 'cream': 153,\n",
       " 'cream_cheese_': 32,\n",
       " 'curd': 25,\n",
       " 'curd_cheese': 119,\n",
       " 'decalcifier': 152,\n",
       " 'dental_care': 110,\n",
       " 'dessert': 36,\n",
       " 'detergent': 59,\n",
       " 'dish_cleaner': 84,\n",
       " 'dishes': 65,\n",
       " 'dog_food': 94,\n",
       " 'domestic_eggs': 19,\n",
       " 'female_sanitary_products': 108,\n",
       " 'finished_products': 106,\n",
       " 'fish': 136,\n",
       " 'flour': 66,\n",
       " 'flower_(seeds)': 85,\n",
       " 'flower_soil/fertilizer': 147,\n",
       " 'frankfurter': 20,\n",
       " 'frozen_chicken': 162,\n",
       " 'frozen_dessert': 83,\n",
       " 'frozen_fish': 77,\n",
       " 'frozen_fruits': 154,\n",
       " 'frozen_meals': 46,\n",
       " 'frozen_potato_products': 96,\n",
       " 'frozen_vegetables': 29,\n",
       " 'fruit/vegetable_juice': 16,\n",
       " 'grapes': 57,\n",
       " 'hair_spray': 155,\n",
       " 'ham': 51,\n",
       " 'hamburger_meat': 39,\n",
       " 'hard_cheese': 55,\n",
       " 'herbs': 70,\n",
       " 'honey': 151,\n",
       " 'house_keeping_products': 97,\n",
       " 'hygiene_articles': 41,\n",
       " 'ice_cream': 53,\n",
       " 'instant_coffee': 102,\n",
       " 'jam': 115,\n",
       " 'ketchup': 123,\n",
       " 'kitchen_towels': 109,\n",
       " 'kitchen_utensil': 165,\n",
       " 'light_bulbs': 125,\n",
       " 'liqueur': 157,\n",
       " 'liquor': 80,\n",
       " 'liquor_(appetizer)': 100,\n",
       " 'liver_loaf': 117,\n",
       " 'long_life_bakery_product': 35,\n",
       " 'make_up_remover': 160,\n",
       " 'male_cosmetics': 120,\n",
       " 'margarine': 21,\n",
       " 'mayonnaise': 90,\n",
       " 'meat': 52,\n",
       " 'meat_spreads': 122,\n",
       " 'misc._beverages': 45,\n",
       " 'mustard': 76,\n",
       " 'napkins': 27,\n",
       " 'newspapers': 13,\n",
       " 'nut_snack': 134,\n",
       " 'nuts/prunes': 130,\n",
       " 'oil': 47,\n",
       " 'onions': 42,\n",
       " 'organic_products': 150,\n",
       " 'organic_sausage': 145,\n",
       " 'other_vegetables': 1,\n",
       " 'packaged_fruit/vegetables': 75,\n",
       " 'pasta': 71,\n",
       " 'pastry': 10,\n",
       " 'pet_care': 88,\n",
       " 'photo/film': 89,\n",
       " 'pickled_vegetables': 62,\n",
       " 'pip_fruit': 15,\n",
       " 'popcorn': 103,\n",
       " 'pork': 23,\n",
       " 'pot_plants': 67,\n",
       " 'potato_products': 137,\n",
       " 'preservation_products': 166,\n",
       " 'processed_cheese': 69,\n",
       " 'prosecco': 146,\n",
       " 'pudding_powder': 143,\n",
       " 'ready_soups': 148,\n",
       " 'red/blush_wine': 60,\n",
       " 'rice': 101,\n",
       " 'roll_products_': 87,\n",
       " 'rolls/buns': 2,\n",
       " 'root_vegetables': 6,\n",
       " 'rubbing_alcohol': 156,\n",
       " 'rum': 121,\n",
       " 'salad_dressing': 158,\n",
       " 'salt': 82,\n",
       " 'salty_snack': 34,\n",
       " 'sauces': 113,\n",
       " 'sausage': 9,\n",
       " 'seasonal_products': 73,\n",
       " 'semi-finished_bread': 64,\n",
       " 'shopping_bags': 8,\n",
       " 'skin_care': 129,\n",
       " 'sliced_cheese': 54,\n",
       " 'snack_products': 135,\n",
       " 'soap': 140,\n",
       " 'soda': 3,\n",
       " 'soft_cheese': 68,\n",
       " 'softener': 114,\n",
       " 'sound_storage_medium': 168,\n",
       " 'soups': 105,\n",
       " 'sparkling_wine': 112,\n",
       " 'specialty_bar': 49,\n",
       " 'specialty_cheese': 95,\n",
       " 'specialty_chocolate': 43,\n",
       " 'specialty_fat': 127,\n",
       " 'specialty_vegetables': 149,\n",
       " 'spices': 116,\n",
       " 'spread_cheese': 79,\n",
       " 'sugar': 37,\n",
       " 'sweet_spreads': 92,\n",
       " 'syrup': 132,\n",
       " 'tea': 126,\n",
       " 'tidbits': 142,\n",
       " 'toilet_cleaner': 161,\n",
       " 'tropical_fruit': 7,\n",
       " 'turkey': 98,\n",
       " 'vinegar': 107,\n",
       " 'waffles': 33,\n",
       " 'whipped/sour_cream': 17,\n",
       " 'whisky': 159,\n",
       " 'white_bread': 31,\n",
       " 'white_wine': 61,\n",
       " 'whole_milk': 0,\n",
       " 'yogurt': 4,\n",
       " 'zwieback': 104}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['UNKNOWN_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, sent in enumerate(basket):\n",
    "    basket[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in basket])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in basket])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "tropical_fruit other_vegetables white_bread bottled_water\n",
      "[7, 1, 31, 5]\n",
      "\n",
      "y:\n",
      "other_vegetables white_bread bottled_water chocolate\n",
      "[1, 31, 5, 28]\n"
     ]
    }
   ],
   "source": [
    "x_example, y_example = X_train[10], y_train[10]\n",
    "print(\"x:\\n%s\\n%s\" % (\" \".join([wordlist[x] for x in x_example]), x_example))\n",
    "print(\"\\ny:\\n%s\\n%s\" % (\" \".join([wordlist[x] for x in y_example]), y_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9835, (9835,))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11, 64, 21], [7, 4], [], [15, 4, 32], [1, 0, 86], [0, 24, 4, 101],\n",
       "       [], [1, 38, 2, 12], [], [0]], dtype=object)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[64, 21, 148], [4, 22], [], [4, 32, 122], [0, 86, 35],\n",
       "       [24, 4, 101, 128], [], [38, 2, 12, 100], [], [111]], dtype=object)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.array([var for var in X_train if var])\n",
    "y_train = np.array([var for var in y_train if var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7676, 7676)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7676,)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tropical_fruit',\n",
       " 'other_vegetables',\n",
       " 'white_bread',\n",
       " 'bottled_water',\n",
       " 'chocolate']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basket[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.3684210526315788"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average([len(row) for row in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "959"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [len(row) for row in X_train]\n",
    "a.index(max(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "x = X_train\n",
    "y = y_train\n",
    "max_length =  10\n",
    "x_padded = []\n",
    "y_padded = []\n",
    "\n",
    "\n",
    "for row in x:\n",
    "    if len(row) <= max_length:\n",
    "        x_padded.append(row + [169] * (max_length - len(row)))\n",
    "    else :\n",
    "        x_padded.append(row[:10])\n",
    "\n",
    "for row in y:\n",
    "    if len(row) <= max_length:\n",
    "        y_padded.append(row + [169] * (max_length - len(row))) \n",
    "    else: \n",
    "        y_padded.append(row[:10])\n",
    "\n",
    "x_padded = np.array(x_padded)\n",
    "y_padded = np.array(y_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7676, 10), (7676, 10))"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_padded.shape, y_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 64,  21, 148, ..., 169, 169, 169],\n",
       "       [  4,  22, 169, ..., 169, 169, 169],\n",
       "       [  4,  32, 122, ..., 169, 169, 169],\n",
       "       ..., \n",
       "       [ 11,   1,  24, ..., 121,  78, 169],\n",
       "       [  5,   3,  12, ..., 169, 169, 169],\n",
       "       [  7,   1, 107, ..., 169, 169, 169]])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7676, 7676)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_padded),len(y_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain, ytrain = x_padded[:7000], y_padded[:7000]\n",
    "xtest, ytest = x_padded[7000:7600], y_padded[7000:7600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "from tensorflow.contrib import rnn \n",
    "hm_epochs = 150\n",
    "batch_size = 500\n",
    "num_steps = 10\n",
    "state_size = 200\n",
    "vocab_size = 170\n",
    "\n",
    "# Placeholders\n",
    "x = tf.placeholder(tf.int32, [None, num_steps]) # [batch_size, num_steps]\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "y = tf.placeholder(tf.int32, [None, num_steps])\n",
    "\n",
    "def train_neural_network(x): \n",
    "    \n",
    "        def next_batch(step):\n",
    "            p,q = xtrain[batch_size*step:batch_size*(step+1)], ytrain[batch_size*step:batch_size*(step+1)]\n",
    "            return p,q\n",
    "        \n",
    "        def test_batch(stp):    \n",
    "            a,b = xtest[batch_size*stp:batch_size*(stp+1)], ytest[batch_size*stp:batch_size*(stp+1)]\n",
    "            return a,b\n",
    "              \n",
    "        def lstm_neural_network(x):\n",
    "            # Embedding layer\n",
    "            rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "            rnn_inputs = tf.cast(rnn_inputs, dtype=tf.float32)\n",
    "            print(rnn_inputs)\n",
    "            \n",
    "            # RNN\n",
    "            inputs = tf.unstack(rnn_inputs, num=num_steps, axis=1)   \n",
    "            cell = tf.contrib.rnn.GRUCell(state_size)\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell=cell, output_keep_prob = 0.5)\n",
    "            print(\"this is rnn going in:\", rnn_inputs)\n",
    "            rnn_outputs, rnn_states = tf.contrib.rnn.static_rnn(cell, inputs, dtype=tf.float32)\n",
    "\n",
    "            rnn_output = tf.reshape(tf.concat(axis=1, values=rnn_outputs), [-1, state_size])\n",
    "            print(\"this is the output:\",rnn_output)\n",
    "            softmax_w = tf.get_variable(\"softmax_w\", [state_size, vocab_size], dtype=tf.float32)\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [vocab_size], dtype=tf.float32)\n",
    "            logits = tf.add(tf.matmul(rnn_output, softmax_w),softmax_b, name = \"logits\")\n",
    "            return logits\n",
    "\n",
    "        prediction = lstm_neural_network(x)\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "            [prediction],\n",
    "            [tf.reshape(y[:batch_size], [-1])],\n",
    "            [tf.ones([batch_size * num_steps], dtype=tf.float32)])\n",
    "        cost = tf.reduce_sum(loss) / batch_size\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(cost)   \n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            for epoch in range(hm_epochs):\n",
    "                epoch_loss = 0 \n",
    "                for step in range(int(len(xtrain)/batch_size)):\n",
    "                    epoch_x, epoch_y = next_batch(step)\n",
    "                    _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                    epoch_loss += c\n",
    "                print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "            save_path = saver.save(sess, \"./model\")\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "            for stp in range(int(len(xtest)/batch_size)):\n",
    "                s,u = test_batch(stp)\n",
    "                correct = tf.equal((tf.argmax(prediction,1)),tf.cast(tf.reshape(tf.concat(axis=1, values= u), [-1]),tf.int64))\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "                print( 'Batch #', stp, 'Accuracy by each batch:',accuracy.eval({x: s, y: u}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Cast:0\", shape=(?, 10, 200), dtype=float32)\n",
      "this is rnn going in: Tensor(\"Cast:0\", shape=(?, 10, 200), dtype=float32)\n",
      "this is the output: Tensor(\"Reshape:0\", shape=(?, 200), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-246-1ecf743354b6>:54: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch 0 completed out of 150 loss: 566.010211945\n",
      "Epoch 1 completed out of 150 loss: 355.764188766\n",
      "Epoch 2 completed out of 150 loss: 307.057319641\n",
      "Epoch 3 completed out of 150 loss: 280.411174774\n",
      "Epoch 4 completed out of 150 loss: 261.422828674\n",
      "Epoch 5 completed out of 150 loss: 246.748120308\n",
      "Epoch 6 completed out of 150 loss: 235.012883186\n",
      "Epoch 7 completed out of 150 loss: 226.932684898\n",
      "Epoch 8 completed out of 150 loss: 220.55629921\n",
      "Epoch 9 completed out of 150 loss: 215.752729416\n",
      "Epoch 10 completed out of 150 loss: 211.853147507\n",
      "Epoch 11 completed out of 150 loss: 208.677114487\n",
      "Epoch 12 completed out of 150 loss: 206.18270874\n",
      "Epoch 13 completed out of 150 loss: 204.025616646\n",
      "Epoch 14 completed out of 150 loss: 202.317617416\n",
      "Epoch 15 completed out of 150 loss: 200.359949112\n",
      "Epoch 16 completed out of 150 loss: 199.311432838\n",
      "Epoch 17 completed out of 150 loss: 198.056500435\n",
      "Epoch 18 completed out of 150 loss: 197.095933914\n",
      "Epoch 19 completed out of 150 loss: 196.24783802\n",
      "Epoch 20 completed out of 150 loss: 195.547118187\n",
      "Epoch 21 completed out of 150 loss: 194.036834717\n",
      "Epoch 22 completed out of 150 loss: 193.409518242\n",
      "Epoch 23 completed out of 150 loss: 192.651799202\n",
      "Epoch 24 completed out of 150 loss: 192.084583282\n",
      "Epoch 25 completed out of 150 loss: 191.65991497\n",
      "Epoch 26 completed out of 150 loss: 190.721703529\n",
      "Epoch 27 completed out of 150 loss: 190.669817924\n",
      "Epoch 28 completed out of 150 loss: 190.273769379\n",
      "Epoch 29 completed out of 150 loss: 189.298409462\n",
      "Epoch 30 completed out of 150 loss: 189.155686378\n",
      "Epoch 31 completed out of 150 loss: 188.371004105\n",
      "Epoch 32 completed out of 150 loss: 188.130679131\n",
      "Epoch 33 completed out of 150 loss: 187.505998611\n",
      "Epoch 34 completed out of 150 loss: 187.139447212\n",
      "Epoch 35 completed out of 150 loss: 186.77031517\n",
      "Epoch 36 completed out of 150 loss: 186.49779892\n",
      "Epoch 37 completed out of 150 loss: 186.045843124\n",
      "Epoch 38 completed out of 150 loss: 185.846509933\n",
      "Epoch 39 completed out of 150 loss: 185.230110168\n",
      "Epoch 40 completed out of 150 loss: 185.000619888\n",
      "Epoch 41 completed out of 150 loss: 184.706958771\n",
      "Epoch 42 completed out of 150 loss: 184.21274662\n",
      "Epoch 43 completed out of 150 loss: 183.849748611\n",
      "Epoch 44 completed out of 150 loss: 183.728591919\n",
      "Epoch 45 completed out of 150 loss: 183.58489418\n",
      "Epoch 46 completed out of 150 loss: 182.82138443\n",
      "Epoch 47 completed out of 150 loss: 182.581335068\n",
      "Epoch 48 completed out of 150 loss: 182.425819397\n",
      "Epoch 49 completed out of 150 loss: 181.964561462\n",
      "Epoch 50 completed out of 150 loss: 181.655268669\n",
      "Epoch 51 completed out of 150 loss: 181.329001427\n",
      "Epoch 52 completed out of 150 loss: 181.214586258\n",
      "Epoch 53 completed out of 150 loss: 180.679286957\n",
      "Epoch 54 completed out of 150 loss: 180.291031837\n",
      "Epoch 55 completed out of 150 loss: 180.078410149\n",
      "Epoch 56 completed out of 150 loss: 179.505781174\n",
      "Epoch 57 completed out of 150 loss: 179.259298325\n",
      "Epoch 58 completed out of 150 loss: 179.157708168\n",
      "Epoch 59 completed out of 150 loss: 178.894514084\n",
      "Epoch 60 completed out of 150 loss: 178.307902336\n",
      "Epoch 61 completed out of 150 loss: 178.275868416\n",
      "Epoch 62 completed out of 150 loss: 177.32324791\n",
      "Epoch 63 completed out of 150 loss: 177.395784378\n",
      "Epoch 64 completed out of 150 loss: 177.146263123\n",
      "Epoch 65 completed out of 150 loss: 176.247797966\n",
      "Epoch 66 completed out of 150 loss: 176.126768112\n",
      "Epoch 67 completed out of 150 loss: 175.87246418\n",
      "Epoch 68 completed out of 150 loss: 175.210836411\n",
      "Epoch 69 completed out of 150 loss: 174.747835159\n",
      "Epoch 70 completed out of 150 loss: 174.485440254\n",
      "Epoch 71 completed out of 150 loss: 174.1191082\n",
      "Epoch 72 completed out of 150 loss: 173.968324661\n",
      "Epoch 73 completed out of 150 loss: 173.351858139\n",
      "Epoch 74 completed out of 150 loss: 172.770281792\n",
      "Epoch 75 completed out of 150 loss: 172.901694298\n",
      "Epoch 76 completed out of 150 loss: 172.256132126\n",
      "Epoch 77 completed out of 150 loss: 171.597172737\n",
      "Epoch 78 completed out of 150 loss: 171.474742889\n",
      "Epoch 79 completed out of 150 loss: 170.787908554\n",
      "Epoch 80 completed out of 150 loss: 170.466604233\n",
      "Epoch 81 completed out of 150 loss: 170.066439629\n",
      "Epoch 82 completed out of 150 loss: 169.908480644\n",
      "Epoch 83 completed out of 150 loss: 169.093736649\n",
      "Epoch 84 completed out of 150 loss: 168.656711578\n",
      "Epoch 85 completed out of 150 loss: 168.380580902\n",
      "Epoch 86 completed out of 150 loss: 167.552972794\n",
      "Epoch 87 completed out of 150 loss: 167.504669189\n",
      "Epoch 88 completed out of 150 loss: 166.900501251\n",
      "Epoch 89 completed out of 150 loss: 166.095283508\n",
      "Epoch 90 completed out of 150 loss: 166.065558434\n",
      "Epoch 91 completed out of 150 loss: 165.337471008\n",
      "Epoch 92 completed out of 150 loss: 164.947073936\n",
      "Epoch 93 completed out of 150 loss: 164.302564621\n",
      "Epoch 94 completed out of 150 loss: 164.230825424\n",
      "Epoch 95 completed out of 150 loss: 163.503040314\n",
      "Epoch 96 completed out of 150 loss: 163.24452877\n",
      "Epoch 97 completed out of 150 loss: 162.787952423\n",
      "Epoch 98 completed out of 150 loss: 162.326256752\n",
      "Epoch 99 completed out of 150 loss: 161.598446846\n",
      "Epoch 100 completed out of 150 loss: 161.162098885\n",
      "Epoch 101 completed out of 150 loss: 160.795953751\n",
      "Epoch 102 completed out of 150 loss: 160.302885056\n",
      "Epoch 103 completed out of 150 loss: 159.6777668\n",
      "Epoch 104 completed out of 150 loss: 158.929782867\n",
      "Epoch 105 completed out of 150 loss: 158.665018082\n",
      "Epoch 106 completed out of 150 loss: 158.438926697\n",
      "Epoch 107 completed out of 150 loss: 157.964945793\n",
      "Epoch 108 completed out of 150 loss: 157.690414429\n",
      "Epoch 109 completed out of 150 loss: 156.996388435\n",
      "Epoch 110 completed out of 150 loss: 156.53731823\n",
      "Epoch 111 completed out of 150 loss: 156.530087471\n",
      "Epoch 112 completed out of 150 loss: 155.419493675\n",
      "Epoch 113 completed out of 150 loss: 155.17562294\n",
      "Epoch 114 completed out of 150 loss: 154.842690468\n",
      "Epoch 115 completed out of 150 loss: 154.092166901\n",
      "Epoch 116 completed out of 150 loss: 153.882907867\n",
      "Epoch 117 completed out of 150 loss: 153.721587181\n",
      "Epoch 118 completed out of 150 loss: 152.926634789\n",
      "Epoch 119 completed out of 150 loss: 152.95031929\n",
      "Epoch 120 completed out of 150 loss: 152.652655602\n",
      "Epoch 121 completed out of 150 loss: 153.263842583\n",
      "Epoch 122 completed out of 150 loss: 153.477762222\n",
      "Epoch 123 completed out of 150 loss: 153.524918556\n",
      "Epoch 124 completed out of 150 loss: 153.463747978\n",
      "Epoch 125 completed out of 150 loss: 153.726755142\n",
      "Epoch 126 completed out of 150 loss: 153.158390045\n",
      "Epoch 127 completed out of 150 loss: 152.895136833\n",
      "Epoch 128 completed out of 150 loss: 151.697938919\n",
      "Epoch 129 completed out of 150 loss: 149.562414169\n",
      "Epoch 130 completed out of 150 loss: 147.887823105\n",
      "Epoch 131 completed out of 150 loss: 147.555236816\n",
      "Epoch 132 completed out of 150 loss: 146.85507679\n",
      "Epoch 133 completed out of 150 loss: 146.5571661\n",
      "Epoch 134 completed out of 150 loss: 146.276068687\n",
      "Epoch 135 completed out of 150 loss: 145.781071663\n",
      "Epoch 136 completed out of 150 loss: 145.213015556\n",
      "Epoch 137 completed out of 150 loss: 144.515766144\n",
      "Epoch 138 completed out of 150 loss: 144.549589157\n",
      "Epoch 139 completed out of 150 loss: 144.490722656\n",
      "Epoch 140 completed out of 150 loss: 144.265481949\n",
      "Epoch 141 completed out of 150 loss: 143.518276215\n",
      "Epoch 142 completed out of 150 loss: 143.267343521\n",
      "Epoch 143 completed out of 150 loss: 143.667845726\n",
      "Epoch 144 completed out of 150 loss: 143.104158401\n",
      "Epoch 145 completed out of 150 loss: 143.26651001\n",
      "Epoch 146 completed out of 150 loss: 142.966876984\n",
      "Epoch 147 completed out of 150 loss: 143.873275757\n",
      "Epoch 148 completed out of 150 loss: 143.778264999\n",
      "Epoch 149 completed out of 150 loss: 144.201691628\n",
      "Model saved in file: ./model\n",
      "Batch # 0 Accuracy by each batch: 0.6276\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are results: [[-0.06714499  0.03206809  0.50385636 ...,  0.0839908   0.28306127\n",
      "   0.07631007]\n",
      " [-0.06654603 -0.15623781  0.32606471 ...,  0.44879982 -0.25671446\n",
      "  -0.10963421]\n",
      " [-0.51463908 -0.35265481  0.40559691 ...,  1.10341454 -0.22839133\n",
      "  -0.40540844]\n",
      " ..., \n",
      " [-0.13943213  0.06104822  0.24092276 ..., -0.12281565  0.05632129\n",
      "   0.17111006]\n",
      " [-0.19139811  0.00268919  0.31316239 ...,  0.00463292  0.12506922\n",
      "   0.04600501]\n",
      " [-0.02362924 -0.0138546   0.22703174 ..., -0.04348115  0.13229501\n",
      "   0.23531948]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sess=tf.Session()    \n",
    "#First let's load meta graph and restore weights\n",
    "saver = tf.train.import_meta_graph('model.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "#context = graph.get_tensor_by_name(\"context:0\") \n",
    "#utterance = graph.get_tensor_by_name(\"utterance:0\")\n",
    "#target = graph.get_tensor_by_name(\"target:0\")\n",
    "\n",
    "op_to_restore = graph.get_tensor_by_name(\"logits:0\")\n",
    "#qe,an,lbl = test_batch(0)\n",
    "feed_dict={x: x_padded[7606:7676]}\n",
    "results = sess.run(op_to_restore,feed_dict)\n",
    "print(\"These are results:\", results)\n",
    "results = np.reshape(results, [70, num_steps, vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r1= results.argmax(axis=2)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r2=y_padded[7606:7676][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x: These are the predictions made \n",
      "seasonal_products,toilet_cleaner,baby_food,abrasive_cleaner,pot_plants,flour,liver_loaf,flour,flour,canned_vegetables\n",
      "\n",
      "y These are the actual present in dataset:\n",
      "whole_milk,whipped/sour_cream,sliced_cheese,domestic_eggs,rolls/buns,margarine,bottled_water,fruit/vegetable_juice,UNKNOWN_TOKEN,UNKNOWN_TOKEN\n",
      "\n",
      "x: These are the predictions made \n",
      "brown_bread,specialty_cheese,frankfurter,brown_bread,brown_bread,cocoa_drinks,canned_vegetables,baby_food,cleaner,brown_bread\n",
      "\n",
      "y These are the actual present in dataset:\n",
      "domestic_eggs,rolls/buns,brown_bread,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN\n",
      "\n",
      "x: These are the predictions made \n",
      "misc._beverages,brown_bread,berries,mustard,brown_bread,cookware,abrasive_cleaner,fish,abrasive_cleaner,liqueur\n",
      "\n",
      "y These are the actual present in dataset:\n",
      "rolls/buns,coffee,newspapers,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN\n",
      "\n",
      "x: These are the predictions made \n",
      "sausage,herbs,chicken,baby_food,flour,prosecco,baby_food,herbs,prosecco,cake_bar\n",
      "\n",
      "y These are the actual present in dataset:\n",
      "beverages,pastry,newspapers,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN\n",
      "\n",
      "x: These are the predictions made \n",
      "house_keeping_products,mustard,fish,white_wine,baby_food,pastry,female_sanitary_products,beef,beef,beef\n",
      "\n",
      "y These are the actual present in dataset:\n",
      "whole_milk,rolls/buns,canned_vegetables,fruit/vegetable_juice,waffles,chocolate,newspapers,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN\n",
      "\n",
      "x: These are the predictions made \n",
      "specialty_cheese,misc._beverages,cocoa_drinks,berries,bathroom_cleaner,cocoa_drinks,flour,baking_powder,canned_fruit,flour\n",
      "\n",
      "y These are the actual present in dataset:\n",
      "citrus_fruit,softener,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN\n",
      "\n",
      "x: These are the predictions made \n",
      "nuts/prunes,cooking_chocolate,abrasive_cleaner,male_cosmetics,oil,nuts/prunes,misc._beverages,nuts/prunes,liqueur,nuts/prunes\n",
      "\n",
      "y These are the actual present in dataset:\n",
      "frozen_meals,shopping_bags,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN\n",
      "\n",
      "x: These are the predictions made \n",
      "flour,pastry,baby_food,cleaner,herbs,abrasive_cleaner,mustard,mustard,baby_food,waffles\n",
      "\n",
      "y These are the actual present in dataset:\n",
      "rolls/buns,specialty_bar,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN\n",
      "\n",
      "x: These are the predictions made \n",
      "sparkling_wine,specialty_bar,kitchen_towels,abrasive_cleaner,cling_film/bags,abrasive_cleaner,canned_fruit,make_up_remover,prosecco,curd\n",
      "\n",
      "y These are the actual present in dataset:\n",
      "other_vegetables,yogurt,brown_bread,pastry,cat_food,abrasive_cleaner,dishes,UNKNOWN_TOKEN,UNKNOWN_TOKEN,UNKNOWN_TOKEN\n",
      "\n",
      "x: These are the predictions made \n",
      "chocolate,dish_cleaner,sweet_spreads,nuts/prunes,instant_coffee,baby_food,sausage,baby_food,cleaner,abrasive_cleaner\n",
      "\n",
      "y These are the actual present in dataset:\n",
      "tropical_fruit,other_vegetables,whole_milk,soft_cheese,curd_cheese,cat_food,hygiene_articles,shopping_bags,UNKNOWN_TOKEN,UNKNOWN_TOKEN\n"
     ]
    }
   ],
   "source": [
    "for pre,the in zip(r1,r2):\n",
    "    print(\"\\nx: These are the predictions made \\n%s\" % (\",\".join([wordlist[x] for x in pre])))\n",
    "    print(\"\\ny These are the actual present in dataset:\\n%s\" % (\",\".join([wordlist[x] for x in the])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
