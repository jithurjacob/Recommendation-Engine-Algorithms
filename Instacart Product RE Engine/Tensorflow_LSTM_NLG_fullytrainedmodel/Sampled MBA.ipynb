{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../RE instacart/datasets/df_order_products__prior_sample.csv\")[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>order_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>add_to_cart_order</th>\n",
       "      <th>reordered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61</td>\n",
       "      <td>8</td>\n",
       "      <td>23423</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>333</td>\n",
       "      <td>40</td>\n",
       "      <td>10070</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>334</td>\n",
       "      <td>40</td>\n",
       "      <td>42450</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>335</td>\n",
       "      <td>40</td>\n",
       "      <td>33198</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>336</td>\n",
       "      <td>40</td>\n",
       "      <td>34866</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  order_id  product_id  add_to_cart_order  reordered\n",
       "0          61         8       23423                  1          1\n",
       "1         333        40       10070                  1          1\n",
       "2         334        40       42450                  2          1\n",
       "3         335        40       33198                  3          1\n",
       "4         336        40       34866                  4          1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products = []\n",
    "for i in df[\"order_id\"].unique():\n",
    "    products.append(list(df[df.order_id == i]['product_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5003"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_products = pd.read_csv(\"../RE instacart/datasets/products.csv\")\n",
    "prod_map = dict(zip(df_products[\"product_id\"],df_products[\"product_name\"]))\n",
    "def getProductName(product_id):   \n",
    "    return prod_map.get(product_id,\"MissingId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "items = [ [getProductName(prod_id) for prod_id in bucket] for bucket in produts]\n",
    "#for bucket in products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Original Hawaiian Sweet Rolls'],\n",
       " ['Organic 1% Low Fat Milk',\n",
       "  'Macaroni & Cheese',\n",
       "  'Sparkling Natural Mineral Water',\n",
       "  'Chocolate Milk 1% Milkfat'],\n",
       " ['Caramel Vanilla Cream Light Roast K-Cup Packs Arabica Coffee',\n",
       "  'Half And Half Ultra Pasteurized',\n",
       "  'Purified Water',\n",
       "  'Ultra Soft & Strong® Toilet Paper Double Rolls',\n",
       "  'Smoothies, Strawberries Wild'],\n",
       " ['Bag of Organic Bananas',\n",
       "  'Plain Greek Yogurt',\n",
       "  'Maple Glazed Honey Ham',\n",
       "  'Organic Grade A Large Brown Eggs',\n",
       "  'Cherry Garcia Ice Cream',\n",
       "  'Prosciutto',\n",
       "  'Thick & Crispy Tortilla Chips',\n",
       "  'High Pulp Orange Juice'],\n",
       " ['Organic Ginger Root',\n",
       "  'Organic Chicken Bone Broth',\n",
       "  'Organic Low Sodium Chicken Cooking Stock',\n",
       "  'Original Soy Creamer',\n",
       "  'Liquid Egg Whites',\n",
       "  'Americone Dream® Ice Cream',\n",
       "  'Rosemary Ham',\n",
       "  'Organic Mayonnaise',\n",
       "  'Organic Sunday Bacon',\n",
       "  'Organic Beefsteak Tomato',\n",
       "  'Organic Roma Tomato']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Instadata.txt', 'wb') as f:\n",
    "    pickle.dump(items, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('embeddings.txt','rb') as fp:\n",
    "    embeddings = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('words.txt', 'rb') as fp:\n",
    "    wordlist = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordlist.append('UNKNOWN_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings = np.vstack([embeddings, np.zeros(200)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11207, 11207)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordlist), len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_index = dict([(w,i) for i,w in enumerate(wordlist)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11206"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['UNKNOWN_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basket = items.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unknown_token = 'UNKNOWN_TOKEN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, sent in enumerate(basket):\n",
    "    basket[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in basket])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in basket])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "Organic Half & Half\n",
      "[33]\n",
      "\n",
      "y:\n",
      "Chunky Peanut Butter\n",
      "[2866]\n"
     ]
    }
   ],
   "source": [
    "x_example, y_example = X_train[10], y_train[10]\n",
    "print(\"x:\\n%s\\n%s\" % (\" \".join([wordlist[x] for x in x_example]), x_example))\n",
    "print(\"\\ny:\\n%s\\n%s\" % (\" \".join([wordlist[x] for x in y_example]), y_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5003, (5003,))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[], [288, 157, 68], [4900, 932, 252, 4089],\n",
       "       [1, 267, 3393, 203, 1525, 1377, 475],\n",
       "       [57, 419, 578, 1118, 999, 1095, 1808, 591, 281, 3859],\n",
       "       [10, 511, 4067, 9052, 8288, 6685, 8381, 4998],\n",
       "       [1203, 812, 2706, 2645, 3163],\n",
       "       [79, 2401, 7, 219, 25, 7543, 1160, 596, 8585], [298, 280, 54],\n",
       "       [16, 793, 199, 2513, 207, 58, 555, 12, 804]], dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[], [157, 68, 1945], [932, 252, 4089, 5146],\n",
       "       [267, 3393, 203, 1525, 1377, 475, 2026],\n",
       "       [419, 578, 1118, 999, 1095, 1808, 591, 281, 3859, 148],\n",
       "       [511, 4067, 9052, 8288, 6685, 8381, 4998, 923],\n",
       "       [812, 2706, 2645, 3163, 529],\n",
       "       [2401, 7, 219, 25, 7543, 1160, 596, 8585, 2609], [280, 54, 140],\n",
       "       [793, 199, 2513, 207, 58, 555, 12, 804, 39]], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array([var for var in X_train if var])\n",
    "y_train = np.array([var for var in y_train if var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "x = X_train\n",
    "y = y_train\n",
    "max_length =  10\n",
    "x_padded = []\n",
    "y_padded = []\n",
    "\n",
    "\n",
    "for row in x:\n",
    "    if len(row) <= max_length:\n",
    "        x_padded.append(row + [word_to_index['UNKNOWN_TOKEN']] * (max_length - len(row)))\n",
    "    else :\n",
    "        x_padded.append(row[:10])\n",
    "\n",
    "for row in y:\n",
    "    if len(row) <= max_length:\n",
    "        y_padded.append(row + [word_to_index['UNKNOWN_TOKEN']] * (max_length - len(row))) \n",
    "    else: \n",
    "        y_padded.append(row[:10])\n",
    "\n",
    "x_padded = np.array(x_padded)\n",
    "y_padded = np.array(y_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4749, 10), (4749, 10))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_padded.shape, y_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain, ytrain = x_padded[:4000], y_padded[:4000]\n",
    "xtest, ytest = x_padded[4000:], y_padded[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "from tensorflow.contrib import rnn \n",
    "hm_epochs = 150\n",
    "batch_size = 500\n",
    "num_steps = 10\n",
    "state_size = 200\n",
    "vocab_size = len(wordlist)\n",
    "\n",
    "# Placeholders\n",
    "x = tf.placeholder(tf.int32, [None, num_steps]) # [batch_size, num_steps]\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "y = tf.placeholder(tf.int32, [None, num_steps])\n",
    "\n",
    "def train_neural_network(x): \n",
    "    \n",
    "        def next_batch(step):\n",
    "            p,q = xtrain[batch_size*step:batch_size*(step+1)], ytrain[batch_size*step:batch_size*(step+1)]\n",
    "            return p,q\n",
    "        \n",
    "        def test_batch(stp):    \n",
    "            a,b = xtest[batch_size*stp:batch_size*(stp+1)], ytest[batch_size*stp:batch_size*(stp+1)]\n",
    "            return a,b\n",
    "              \n",
    "        def lstm_neural_network(x):\n",
    "            # Embedding layer\n",
    "            rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "            rnn_inputs = tf.cast(rnn_inputs, dtype=tf.float32)\n",
    "            print(rnn_inputs)\n",
    "            \n",
    "            # RNN\n",
    "            inputs = tf.unstack(rnn_inputs, num=num_steps, axis=1)   \n",
    "            cell = tf.contrib.rnn.GRUCell(state_size)\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell=cell, output_keep_prob = 0.5)\n",
    "            print(\"this is rnn going in:\", rnn_inputs)\n",
    "            rnn_outputs, rnn_states = tf.contrib.rnn.static_rnn(cell, inputs, dtype=tf.float32)\n",
    "\n",
    "            rnn_output = tf.reshape(tf.concat(axis=1, values=rnn_outputs), [-1, state_size])\n",
    "            print(\"this is the output:\",rnn_output)\n",
    "            softmax_w = tf.get_variable(\"softmax_w\", [state_size, vocab_size], dtype=tf.float32)\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [vocab_size], dtype=tf.float32)\n",
    "            logits = tf.add(tf.matmul(rnn_output, softmax_w),softmax_b, name = \"logits\")\n",
    "            return logits\n",
    "\n",
    "        prediction = lstm_neural_network(x)\n",
    "        loss = tf.argmax(prediction, axis=1)\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "            [prediction],\n",
    "            [tf.reshape(y[:batch_size], [-1])],\n",
    "            [tf.ones([batch_size * num_steps], dtype=tf.float32)])\n",
    "        cost = tf.reduce_sum(loss) / batch_size\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(cost)   \n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            for epoch in range(hm_epochs):\n",
    "                epoch_loss = 0 \n",
    "                for step in range(int(len(xtrain)/batch_size)):\n",
    "                    epoch_x, epoch_y = next_batch(step)\n",
    "                    _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                    epoch_loss += c\n",
    "                print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "            save_path = saver.save(sess, \"./model\")\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "            for stp in range(int(len(xtest)/batch_size)):\n",
    "                s,u = test_batch(stp)\n",
    "                correct = tf.equal((tf.argmax(prediction,1)),tf.cast(tf.reshape(tf.concat(axis=1, values= u), [-1]),tf.int64))\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "                print( 'Batch #', stp, 'Accuracy by each batch:',accuracy.eval({x: s, y: u}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Cast:0\", shape=(?, 10, 200), dtype=float32)\n",
      "this is rnn going in: Tensor(\"Cast:0\", shape=(?, 10, 200), dtype=float32)\n",
      "this is the output: Tensor(\"Reshape:0\", shape=(?, 200), dtype=float32)\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch 0 completed out of 150 loss: 743.870384216\n",
      "Epoch 1 completed out of 150 loss: 719.909934998\n",
      "Epoch 2 completed out of 150 loss: 627.529769897\n",
      "Epoch 3 completed out of 150 loss: 536.962463379\n",
      "Epoch 4 completed out of 150 loss: 514.900238037\n",
      "Epoch 5 completed out of 150 loss: 509.803375244\n",
      "Epoch 6 completed out of 150 loss: 504.931190491\n",
      "Epoch 7 completed out of 150 loss: 502.242801666\n",
      "Epoch 8 completed out of 150 loss: 500.407283783\n",
      "Epoch 9 completed out of 150 loss: 498.887355804\n",
      "Epoch 10 completed out of 150 loss: 497.759822845\n",
      "Epoch 11 completed out of 150 loss: 496.418159485\n",
      "Epoch 12 completed out of 150 loss: 495.040977478\n",
      "Epoch 13 completed out of 150 loss: 493.514068604\n",
      "Epoch 14 completed out of 150 loss: 491.952629089\n",
      "Epoch 15 completed out of 150 loss: 490.513149261\n",
      "Epoch 16 completed out of 150 loss: 488.702465057\n",
      "Epoch 17 completed out of 150 loss: 486.814743042\n",
      "Epoch 18 completed out of 150 loss: 485.065643311\n",
      "Epoch 19 completed out of 150 loss: 482.993980408\n",
      "Epoch 20 completed out of 150 loss: 481.174144745\n",
      "Epoch 21 completed out of 150 loss: 479.113891602\n",
      "Epoch 22 completed out of 150 loss: 477.03327179\n",
      "Epoch 23 completed out of 150 loss: 475.136951447\n",
      "Epoch 24 completed out of 150 loss: 473.04914093\n",
      "Epoch 25 completed out of 150 loss: 470.886940002\n",
      "Epoch 26 completed out of 150 loss: 469.040416718\n",
      "Epoch 27 completed out of 150 loss: 466.909305573\n",
      "Epoch 28 completed out of 150 loss: 465.019687653\n",
      "Epoch 29 completed out of 150 loss: 462.771270752\n",
      "Epoch 30 completed out of 150 loss: 460.548511505\n",
      "Epoch 31 completed out of 150 loss: 458.218971252\n",
      "Epoch 32 completed out of 150 loss: 455.917331696\n",
      "Epoch 33 completed out of 150 loss: 453.748703003\n",
      "Epoch 34 completed out of 150 loss: 451.24370575\n",
      "Epoch 35 completed out of 150 loss: 448.982261658\n",
      "Epoch 36 completed out of 150 loss: 446.529380798\n",
      "Epoch 37 completed out of 150 loss: 443.842987061\n",
      "Epoch 38 completed out of 150 loss: 440.984237671\n",
      "Epoch 39 completed out of 150 loss: 438.086856842\n",
      "Epoch 40 completed out of 150 loss: 435.1926651\n",
      "Epoch 41 completed out of 150 loss: 431.922264099\n",
      "Epoch 42 completed out of 150 loss: 428.844078064\n",
      "Epoch 43 completed out of 150 loss: 425.692726135\n",
      "Epoch 44 completed out of 150 loss: 421.934677124\n",
      "Epoch 45 completed out of 150 loss: 418.030921936\n",
      "Epoch 46 completed out of 150 loss: 414.303062439\n",
      "Epoch 47 completed out of 150 loss: 410.188915253\n",
      "Epoch 48 completed out of 150 loss: 405.895599365\n",
      "Epoch 49 completed out of 150 loss: 401.818992615\n",
      "Epoch 50 completed out of 150 loss: 397.757930756\n",
      "Epoch 51 completed out of 150 loss: 393.486022949\n",
      "Epoch 52 completed out of 150 loss: 389.200332642\n",
      "Epoch 53 completed out of 150 loss: 385.466644287\n",
      "Epoch 54 completed out of 150 loss: 381.242771149\n",
      "Epoch 55 completed out of 150 loss: 376.50661087\n",
      "Epoch 56 completed out of 150 loss: 372.079143524\n",
      "Epoch 57 completed out of 150 loss: 367.896266937\n",
      "Epoch 58 completed out of 150 loss: 363.205982208\n",
      "Epoch 59 completed out of 150 loss: 359.103042603\n",
      "Epoch 60 completed out of 150 loss: 354.920555115\n",
      "Epoch 61 completed out of 150 loss: 350.573066711\n",
      "Epoch 62 completed out of 150 loss: 346.387248993\n",
      "Epoch 63 completed out of 150 loss: 341.821689606\n",
      "Epoch 64 completed out of 150 loss: 337.591602325\n",
      "Epoch 65 completed out of 150 loss: 333.427337646\n",
      "Epoch 66 completed out of 150 loss: 329.205497742\n",
      "Epoch 67 completed out of 150 loss: 325.238945007\n",
      "Epoch 68 completed out of 150 loss: 320.951789856\n",
      "Epoch 69 completed out of 150 loss: 316.943004608\n",
      "Epoch 70 completed out of 150 loss: 313.086715698\n",
      "Epoch 71 completed out of 150 loss: 308.974506378\n",
      "Epoch 72 completed out of 150 loss: 304.867321014\n",
      "Epoch 73 completed out of 150 loss: 300.825408936\n",
      "Epoch 74 completed out of 150 loss: 296.581020355\n",
      "Epoch 75 completed out of 150 loss: 292.078102112\n",
      "Epoch 76 completed out of 150 loss: 288.100383759\n",
      "Epoch 77 completed out of 150 loss: 283.806591034\n",
      "Epoch 78 completed out of 150 loss: 279.481304169\n",
      "Epoch 79 completed out of 150 loss: 275.544780731\n",
      "Epoch 80 completed out of 150 loss: 271.699516296\n",
      "Epoch 81 completed out of 150 loss: 267.991004944\n",
      "Epoch 82 completed out of 150 loss: 264.282236099\n",
      "Epoch 83 completed out of 150 loss: 260.181613922\n",
      "Epoch 84 completed out of 150 loss: 256.740596771\n",
      "Epoch 85 completed out of 150 loss: 253.004940033\n",
      "Epoch 86 completed out of 150 loss: 249.126249313\n",
      "Epoch 87 completed out of 150 loss: 245.713327408\n",
      "Epoch 88 completed out of 150 loss: 242.410821915\n",
      "Epoch 89 completed out of 150 loss: 239.283670425\n",
      "Epoch 90 completed out of 150 loss: 236.040075302\n",
      "Epoch 91 completed out of 150 loss: 233.264448166\n",
      "Epoch 92 completed out of 150 loss: 230.148925781\n",
      "Epoch 93 completed out of 150 loss: 227.695987701\n",
      "Epoch 94 completed out of 150 loss: 224.309612274\n",
      "Epoch 95 completed out of 150 loss: 221.267469406\n",
      "Epoch 96 completed out of 150 loss: 217.458091736\n",
      "Epoch 97 completed out of 150 loss: 214.938394547\n",
      "Epoch 98 completed out of 150 loss: 212.032585144\n",
      "Epoch 99 completed out of 150 loss: 209.812608719\n",
      "Epoch 100 completed out of 150 loss: 207.187683105\n",
      "Epoch 101 completed out of 150 loss: 204.459280014\n",
      "Epoch 102 completed out of 150 loss: 201.954193115\n",
      "Epoch 103 completed out of 150 loss: 199.175230026\n",
      "Epoch 104 completed out of 150 loss: 196.897911072\n",
      "Epoch 105 completed out of 150 loss: 194.707738876\n",
      "Epoch 106 completed out of 150 loss: 192.164716721\n",
      "Epoch 107 completed out of 150 loss: 189.727746964\n",
      "Epoch 108 completed out of 150 loss: 187.679422379\n",
      "Epoch 109 completed out of 150 loss: 185.58281517\n",
      "Epoch 110 completed out of 150 loss: 183.866577148\n",
      "Epoch 111 completed out of 150 loss: 182.006628036\n",
      "Epoch 112 completed out of 150 loss: 179.898395538\n",
      "Epoch 113 completed out of 150 loss: 178.112365723\n",
      "Epoch 114 completed out of 150 loss: 176.469348907\n",
      "Epoch 115 completed out of 150 loss: 174.936922073\n",
      "Epoch 116 completed out of 150 loss: 172.794555664\n",
      "Epoch 117 completed out of 150 loss: 171.063621521\n",
      "Epoch 118 completed out of 150 loss: 168.709581375\n",
      "Epoch 119 completed out of 150 loss: 167.460769653\n",
      "Epoch 120 completed out of 150 loss: 165.149885178\n",
      "Epoch 121 completed out of 150 loss: 162.784620285\n",
      "Epoch 122 completed out of 150 loss: 161.685083389\n",
      "Epoch 123 completed out of 150 loss: 160.558179855\n",
      "Epoch 124 completed out of 150 loss: 158.880760193\n",
      "Epoch 125 completed out of 150 loss: 156.841487885\n",
      "Epoch 126 completed out of 150 loss: 155.185909271\n",
      "Epoch 127 completed out of 150 loss: 153.350530624\n",
      "Epoch 128 completed out of 150 loss: 151.657114029\n",
      "Epoch 129 completed out of 150 loss: 151.233514786\n",
      "Epoch 130 completed out of 150 loss: 149.008560181\n",
      "Epoch 131 completed out of 150 loss: 147.877639771\n",
      "Epoch 132 completed out of 150 loss: 147.693634033\n",
      "Epoch 133 completed out of 150 loss: 146.098133087\n",
      "Epoch 134 completed out of 150 loss: 144.411720276\n",
      "Epoch 135 completed out of 150 loss: 143.712226868\n",
      "Epoch 136 completed out of 150 loss: 142.337854385\n",
      "Epoch 137 completed out of 150 loss: 141.346176147\n",
      "Epoch 138 completed out of 150 loss: 139.488800049\n",
      "Epoch 139 completed out of 150 loss: 137.683113098\n",
      "Epoch 140 completed out of 150 loss: 136.726268768\n",
      "Epoch 141 completed out of 150 loss: 135.384159088\n",
      "Epoch 142 completed out of 150 loss: 134.496397018\n",
      "Epoch 143 completed out of 150 loss: 133.297447205\n",
      "Epoch 144 completed out of 150 loss: 131.628817558\n",
      "Epoch 145 completed out of 150 loss: 130.791596413\n",
      "Epoch 146 completed out of 150 loss: 129.767317772\n",
      "Epoch 147 completed out of 150 loss: 128.714111328\n",
      "Epoch 148 completed out of 150 loss: 127.782446861\n",
      "Epoch 149 completed out of 150 loss: 126.931970596\n",
      "Model saved in file: ./model\n",
      "Batch # 0 Accuracy by each batch: 0.2924\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4749"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt = np.array(x_padded[4649:4749])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_prod = [[\"California White Jasmine Rice\",\"Squares Dark 60% Cacao Chocolate\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_prod_embeds = [[word_to_index[prod] for prod in bucket] for bucket in test_prod ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3157, 9009]]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prod_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3157, 9009]\n"
     ]
    }
   ],
   "source": [
    "test_x_padded = []\n",
    "for row in test_prod_embeds:    \n",
    "        print(row)\n",
    "        if len(row) <= max_length:\n",
    "            test_x_padded.append(row + [word_to_index['UNKNOWN_TOKEN']] * (max_length - len(row)))\n",
    "        else :\n",
    "            test_x_padded.append(test_prod_embeds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3157, 9009, 11206, 11206, 11206, 11206, 11206, 11206, 11206, 11206]]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_x_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model\n",
      "These are results: [[ 0.00732417  0.0321641   0.05668192 ..., -0.00361386 -0.04422812\n",
      "  -0.02499113]\n",
      " [-0.01966417 -0.00751682  0.05242439 ...,  0.02392079 -0.0143792\n",
      "   0.0313725 ]\n",
      " [-0.02138201 -0.01870059  0.01916721 ..., -0.00908546 -0.06976476\n",
      "   0.00417476]\n",
      " ..., \n",
      " [-0.00337588 -0.01292137  0.02924705 ..., -0.01917645 -0.0014609\n",
      "  -0.00765509]\n",
      " [-0.00208684 -0.00583546  0.01550917 ..., -0.00766807  0.00121961\n",
      "  -0.01017604]\n",
      " [ 0.01157808 -0.01246053  0.01432914 ...,  0.0015014   0.01691655\n",
      "  -0.0150364 ]]\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()    \n",
    "#First let's load meta graph and restore weights\n",
    "saver = tf.train.import_meta_graph('model.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "#context = graph.get_tensor_by_name(\"context:0\") \n",
    "#utterance = graph.get_tensor_by_name(\"utterance:0\")\n",
    "#target = graph.get_tensor_by_name(\"target:0\")\n",
    "\n",
    "op_to_restore = graph.get_tensor_by_name(\"logits:0\")\n",
    "#qe,an,lbl = test_batch(0)\n",
    "feed_dict={x: test_x_padded}\n",
    "#feed_dict={x: tt}\n",
    "results = sess.run(op_to_restore,feed_dict)\n",
    "print(\"These are results:\", results)\n",
    "results = np.reshape(results, [len(test_x_padded), num_steps, vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r1= results.argmax(axis=2)\n",
    "#r2=tt[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(r1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  845,  2477,  8145,  2796,  8625, 10237,  1354,  8263, 10051,\n",
       "         7531]], dtype=int64)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts\n",
      "\n",
      "y': These are the predictions made \n",
      "Salmon Pinwheels\n",
      "End\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pre in (r1[:10]):\n",
    "    print(\"Starts\")\n",
    "    print(\"\\ny': These are the predictions made \\n%s\" % (wordlist[pre[-1]] ))\n",
    "    #print(\"\\ny': These are the predictions made \\n%s\" % (\",\".join([wordlist[x] for x in pre])))\n",
    "    #print(\"\\ny These are the ground truths :\\n%s\" % (\",\".join([wordlist[x] for x in the])))\n",
    "    print(\"End\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
